# Scoring Rubric for CLAUDE.md / agents.md Files

## Criterion 1: Minimality of Requirements

**Excellent (4):** The file is highly concise and describes only the minimal requirements necessary to interact with the repository.

**Satisfactory (3):** The file is mostly concise but includes a few extraneous details that do not significantly impact the agent.

**Needs Improvement (1):** The file includes unnecessary requirements and bloated instructions. Providing non-essential instructions makes tasks harder for coding agents, increasing their exploration time and raising inference costs by over 20%.

### What to look for:
- Line count (under 50 lines = excellent, 50-100 = satisfactory, 100+ = needs improvement)
- Explanatory prose that could be replaced with a single command
- Instructions that state the obvious ("write clean code", "use meaningful names")
- Walls of text where bullet points would suffice
- Sections that exist "just in case" but aren't operationally necessary

---

## Criterion 2: Specification of Tooling and Environment

**Excellent (4):** Explicitly lists specific tooling to be used with the repository (e.g., specific test runners like `pytest`, package managers like `uv`, or custom repository scripts). Data shows that coding agents strictly adhere to these instructions and will reliably use the mentioned tools.

**Satisfactory (3):** Mentions some useful tooling, but omits minor environment-specific commands.

**Needs Improvement (1):** Fails to mention any relevant tools, leaving the agent to blindly guess how to build the project, run tests, or navigate dependencies.

### What to look for:
- Specific, runnable commands (not "run the tests" but `uv run pytest -x`)
- Package manager specified (npm, uv, pnpm, yarn, etc.)
- Test runner and how to run a single test
- Lint/format commands
- Build commands
- Any custom scripts or makefiles
- Environment setup requirements (env vars, Docker, etc.)

---

## Criterion 3: Absence of Codebase Overviews

**Excellent (4):** Intentionally omits exhaustive descriptions of the repository's directories and files. Research shows that codebase overviews in context files are ineffective and do not help agents locate relevant files any faster.

**Satisfactory (3):** Contains a very brief, high-level summary that does not clutter the agent's context window.

**Needs Improvement (1):** Includes explicitly enumerated directories, subdirectories, and extensive file summaries. This can confuse models (such as GPT-4o-mini), causing them to waste steps re-reading the context files and failing to speed up file discovery.

### What to look for:
- Directory trees or file listings (`src/components/`, `src/hooks/`, `src/utils/`, etc.)
- "Architecture Overview" sections that describe what each directory contains
- File-by-file descriptions of what code does
- Technology stack listings that read like a README
- Descriptions of "key components" or "core modules" with file paths

### Acceptable exceptions:
- Mentioning a specific file that has a non-obvious purpose or naming convention (e.g., "DB queries must go through `src/db/queries.ts`")
- Noting files that should NOT be modified (e.g., "`src/generated/` is auto-generated, never edit manually")

---

## Criterion 4: Novelty vs. Redundancy

**Excellent (4):** Provides unique, agent-specific operational context that cannot be found elsewhere in the repository.

**Satisfactory (3):** Contains some overlap with existing documentation, but still offers distinct instructions tailored for an automated agent.

**Needs Improvement (1):** Is highly redundant with existing documentation (such as `README.md` files or `docs/` folders). This is a common flaw in purely LLM-generated context files, which only demonstrate performance benefits when all other repository documentation is actively deleted.

### What to look for:
- Compare content against README.mdâ€”any duplicated sections?
- Does the file describe the project's purpose or history? (belongs in README)
- Does it list the tech stack? (belongs in README)
- Does it describe API endpoints or data models? (belongs in docs/)
- Does it provide setup instructions already in README? (redundant)

### What IS novel and valuable:
- Agent-specific workflow rules (e.g., "create feature branches", "never push to main")
- Gotchas that would trip up an agent but not a human reading docs
- Non-obvious command variations (running a single test vs. full suite)
- Project-specific conventions that contradict language defaults

---

## Criterion 5: Authorship and Curation

**Excellent (4):** Written or heavily curated by a human developer. Developer-provided context files have been shown to yield a marginal performance gain (an average increase of 4% in task resolution rates) while maintaining better alignment with the specific repository.

**Satisfactory (3):** Generated by an LLM but heavily edited and reviewed by a human to remove fluff and enforce the criteria above.

**Needs Improvement (1):** Generated entirely by an LLM (such as using an `/init` command) and left unedited. Contrary to current industry recommendations, using purely LLM-generated context files actually reduces task success rates across models by an average of 3%.

### Signals of LLM-generated content:
- Exhaustive, symmetrical file trees covering every directory
- Generic advice that applies to any project ("write clean, maintainable code")
- Overly structured with identical formatting for every section
- Sections that describe what Claude "should" know vs. what it needs to do
- Boilerplate headers like "About This Project" or "Technology Stack"
- Descriptions of standard tools as if the agent doesn't know them ("PostgreSQL is a relational database")
- Suspiciously thorough coverage of every aspect of the project

### Signals of human-written content:
- Terse, opinionated instructions
- Reflects real pain points and gotchas from development experience
- Inconsistent formatting (humans don't obsess over symmetry)
- Commands that are specific and clearly tested
- Mentions of non-obvious workarounds or quirks
